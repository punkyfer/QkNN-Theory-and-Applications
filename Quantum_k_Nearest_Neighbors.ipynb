{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b03798c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qiskit_aer import Aer\n",
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister, transpile\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.circuit.library import StatePreparation\n",
    "from qiskit_algorithms import AmplificationProblem, Grover\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from qiskit.primitives import Sampler\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "class QuantumKNN:\n",
    "    def normalize_vector(self, vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / norm\n",
    "    \n",
    "    def where_to_apply_x(self, bin_number_length):\n",
    "        powers_of_two = 2 ** np.arange(bin_number_length)\n",
    "        indices = \\\n",
    "            [\n",
    "                [\n",
    "                    ind for ind, v in enumerate(powers_of_two)\n",
    "                    if v & (pos ^ (pos - 1)) == v\n",
    "                ] for pos in range(2 ** bin_number_length)\n",
    "            ]\n",
    "        return indices\n",
    "    \n",
    "    def __init__(self, train_points, train_labels, test_point):\n",
    "        self.train_points = [self.normalize_vector(train_point) for train_point in train_points]\n",
    "        self.train_labels = train_labels\n",
    "        self.test_point = self.normalize_vector(test_point)       \n",
    "        self.o_n = math.ceil(np.log2(len(train_points[0])))  # Number of qubits for indexing training/test points\n",
    "        self.o_m = math.ceil(np.log2(len(train_points)))  # Number of qubits to represent data points\n",
    "        self.quantum_circuit = self.construct_knn()\n",
    "        \n",
    "    def predict(self, num_neighbors):\n",
    "        self.fidelities = self.get_fidelities(self.counts)\n",
    "        label_counts = defaultdict(lambda: 0)\n",
    "        for ix in self.fidelities.argsort()[::-1][:num_neighbors]:\n",
    "            if ix < len(self.train_labels):\n",
    "                label_counts[self.train_labels[ix]] += 1\n",
    "        return max(label_counts, key=label_counts.get)\n",
    "            \n",
    "        \n",
    "    def construct_knn(self):\n",
    "        # Quantum Registers\n",
    "        similarity = QuantumRegister(1, name=\"similarity\")\n",
    "        test_reg = QuantumRegister(self.o_n, name=\"test_reg\")\n",
    "        train_reg = QuantumRegister(self.o_n, name=\"train_reg\")\n",
    "        train_basis = QuantumRegister(self.o_m, name=\"train_basis\")\n",
    "\n",
    "        # State Preparation Circuit without Measurement\n",
    "        state_prep = QuantumCircuit(similarity, test_reg, train_reg, train_basis, name='state_prep')\n",
    "\n",
    "        # State Preparation\n",
    "        state_prep.append(StatePreparation(self.test_point), test_reg[:])\n",
    "        state_prep.h(train_basis)\n",
    "\n",
    "        # Controlled State Preparation\n",
    "        controlled_inits = [StatePreparation(train_state).control(self.o_m) for train_state in self.train_points]\n",
    "\n",
    "        # Apply controlled initializations and X gates\n",
    "        where_x = self.where_to_apply_x(self.o_m)\n",
    "        for i, (c_init, x_idx) in enumerate(zip(controlled_inits, where_x)):\n",
    "            state_prep.x(train_basis[x_idx])\n",
    "            state_prep.append(c_init, train_basis[:] + train_reg[:])\n",
    "            \n",
    "        state_prep.barrier()\n",
    "\n",
    "        # Swap Test\n",
    "        state_prep.h(similarity)\n",
    "        for psi_bit, phi_bit in zip(test_reg, train_reg):\n",
    "            state_prep.cswap(similarity, psi_bit, phi_bit)\n",
    "\n",
    "        state_prep.h(similarity)\n",
    "        \n",
    "        state_prep.barrier()\n",
    "\n",
    "        # Oracle Construction\n",
    "        def construct_oracle(similarity_qubit_index, train_basis_indices, total_qubits):\n",
    "            oracle = QuantumCircuit(total_qubits, name='oracle')\n",
    "\n",
    "            # Apply X gate to similarity qubit to flip it if needed\n",
    "            oracle.x(similarity_qubit_index)\n",
    "\n",
    "            # Initialize auxiliary qubits in superposition\n",
    "            oracle.h(train_basis_indices)\n",
    "\n",
    "            # Apply multi-controlled Z gate to all auxiliary qubits\n",
    "            control_qubits = [similarity_qubit_index]\n",
    "            for aux_bit in train_basis_indices:\n",
    "                oracle.mcp(2*math.pi, control_qubits, aux_bit)\n",
    "\n",
    "            # Return auxiliary qubits to original state\n",
    "            oracle.h(train_basis_indices)\n",
    "\n",
    "            # Apply X gate to similarity qubit to return it to its original state\n",
    "            oracle.x(similarity_qubit_index)\n",
    "\n",
    "            return oracle\n",
    "\n",
    "\n",
    "        # Construct the oracle\n",
    "        similarity_qubit_index = 0\n",
    "        train_basis_indices = list(range(1+self.o_n+self.o_n, 1+self.o_n+self.o_n+self.o_m))\n",
    "        total_qubits = similarity.size + test_reg.size + train_reg.size + train_basis.size\n",
    "        oracle = construct_oracle(similarity_qubit_index, train_basis_indices, total_qubits)\n",
    "\n",
    "        # Grover's Algorithm\n",
    "        iterations = int(np.floor(np.pi / 4 * np.sqrt(2**self.o_m)))\n",
    "        problem = AmplificationProblem(oracle=oracle, state_preparation=state_prep)\n",
    "        grover = Grover(iterations=iterations, sampler=Sampler())  \n",
    "        amplified_circuit = grover.construct_circuit(problem)\n",
    "\n",
    "        # Final Circuit with Measurements\n",
    "        final_circuit = QuantumCircuit(similarity, test_reg, train_reg, train_basis)\n",
    "        #final_circuit.compose(state_prep, inplace=True)\n",
    "        final_circuit.compose(amplified_circuit, inplace=True)\n",
    "\n",
    "        # Classical Register\n",
    "        c_train_basis = ClassicalRegister(self.o_m, name=\"meas_basis\")\n",
    "        c_ancilla = ClassicalRegister(1, name=\"meas_ancilla\")\n",
    "        final_circuit.add_register(c_ancilla)\n",
    "        final_circuit.add_register(c_train_basis)\n",
    "\n",
    "        final_circuit.measure(similarity, c_ancilla)\n",
    "\n",
    "        for qbit, cbit in zip(train_basis, c_train_basis):\n",
    "            final_circuit.measure(qbit, cbit)\n",
    "            \n",
    "        return final_circuit\n",
    "            \n",
    "    def measure(self):\n",
    "        # Execute the circuit\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        tqc = transpile(self.quantum_circuit, backend)\n",
    "        result = backend.run(tqc, shots=1024).result()\n",
    "        self.counts = result.get_counts()\n",
    "            \n",
    "            \n",
    "    def get_fidelities(self, counts):\n",
    "        basis_counts = defaultdict(lambda: 0)\n",
    "        similarity_counts = defaultdict(lambda: 0)\n",
    "        for count_key in counts:\n",
    "            basis, similarity = count_key.split(\" \")\n",
    "            basis_counts[basis] += counts[count_key]\n",
    "            similarity_counts[similarity] += counts[count_key]\n",
    "\n",
    "        exp_fidelity = np.abs(similarity_counts['0'] - similarity_counts['1']) / sum(similarity_counts.values())\n",
    "\n",
    "        num_qubits = len(list(basis_counts.keys())[0])\n",
    "        fidelities = np.zeros(2 ** num_qubits, dtype=float)\n",
    "        for basis_state in basis_counts.keys():\n",
    "            ix = int(basis_state, 2)\n",
    "            for similarity_state in similarity_counts.keys():\n",
    "                state_str = basis_state + ' ' + similarity_state\n",
    "                if state_str in counts:\n",
    "                    fidelities[ix] += \\\n",
    "                        (-1) ** int(similarity_state) * \\\n",
    "                        (counts[state_str]) / similarity_counts[similarity_state] * \\\n",
    "                        (1 - exp_fidelity ** 2)\n",
    "            fidelities[ix] *= 2 ** num_qubits / 2\n",
    "            fidelities[ix] += exp_fidelity\n",
    "        return fidelities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceb241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_neighbors = 3\n",
    "num_points = 8  # Number of training points\n",
    "num_dimensions = 4\n",
    "test_point = np.random.rand(num_dimensions)\n",
    "train_points = [(np.random.rand(num_dimensions)) for _ in range(num_points)]\n",
    "train_labels = [random.randint(0,3) for _ in range(num_points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cce5e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "label_counts = defaultdict(lambda: 0)\n",
    "distances = np.array([math.dist(test_point, train_point) for train_point in train_points])\n",
    "for ix in distances.argsort()[::][:num_neighbors]:\n",
    "    label_counts[train_labels[ix]] += 1\n",
    "pred_label = max(label_counts, key=label_counts.get)\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19195077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_knn = QuantumKNN(train_points, train_labels, test_point)\n",
    "q_knn.measure()\n",
    "q_knn.predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf93b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split , KFold\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e57ba",
   "metadata": {},
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a04cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "# np.c_ is the numpy concatenate function\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                      columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "iris_x= iris_df.iloc[:, :-1] # Data\n",
    "iris_y= iris_df.iloc[:, -1] # Class\n",
    "\n",
    "iris_x_train, iris_x_test, iris_y_train, iris_y_test= train_test_split(iris_x, iris_y,\n",
    "                                                   test_size= 0.14,\n",
    "                                                   shuffle= True, #shuffle the data to avoid bias\n",
    "                                                   random_state= 0)\n",
    "\n",
    "iris_x_train= np.asarray(iris_x_train) # Training Points\n",
    "iris_y_train= np.asarray(iris_y_train) # Labels\n",
    "\n",
    "iris_x_test= np.asarray(iris_x_test[:20])\n",
    "iris_y_test= np.asarray(iris_y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f51dd05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 13.411328554153442\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 56.51359581947327\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 187.17765641212463\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 602.1863422393799\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 2109.985020160675\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 42.55013394355774\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 56.77265024185181\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 187.1464512348175\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 604.7933285236359\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 2114.371343612671\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 43.291598081588745\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 56.5724835395813\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 187.24867701530457\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 603.6078157424927\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 2124.474976539612\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "import time    \n",
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "# np.c_ is the numpy concatenate function\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                      columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "iris_x= iris_df.iloc[:, :-1] # Data\n",
    "iris_y= iris_df.iloc[:, -1] # Class\n",
    "\n",
    "iris_x_train, iris_x_test, iris_y_train, iris_y_test= train_test_split(iris_x, iris_y,\n",
    "                                                   test_size= 0.14,\n",
    "                                                   shuffle= True, #shuffle the data to avoid bias\n",
    "                                                   random_state= 0)\n",
    "\n",
    "iris_x_train= np.asarray(iris_x_train) # Training Points\n",
    "iris_y_train= np.asarray(iris_y_train) # Labels\n",
    "\n",
    "iris_x_test= np.asarray(iris_x_test[:20])\n",
    "iris_y_test= np.asarray(iris_y_test[:20])\n",
    "\n",
    "iris_results_dict = {}\n",
    "training_points = [8, 16, 32, 64, 128]\n",
    "for tp in training_points:\n",
    "    print(f\"Computing {len(iris_x_test)} points with {len(iris_x_train[:tp])} training points, started.\")\n",
    "    pred_y = defaultdict(lambda: [])\n",
    "    q_knn_results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, test_point in enumerate(iris_x_test):\n",
    "        q_knn = QuantumKNN(iris_x_train[:tp], iris_y_train[:tp], test_point)\n",
    "        q_knn.measure()\n",
    "        q_knn_results.append(q_knn)\n",
    "        for nn in range(1, tp+1):\n",
    "            if nn <= tp:\n",
    "                pred_y[str(nn)].append(q_knn.predict(nn))\n",
    "\n",
    "    #q_knn.quantum_circuit.draw('mpl', filename=f\"results/iris_quantum_circuit_{tp}_training_points.png\")\n",
    "    accuracy = defaultdict(lambda: 0)\n",
    "    for pkey in pred_y:\n",
    "        accuracy[pkey] = accuracy_score(iris_y_test, pred_y[pkey])\n",
    "    time_taken = time.time() - start_time\n",
    "    iris_results_dict[str(tp)] = {\n",
    "        \"pred_y\": dict(pred_y),\n",
    "        \"time_taken\": time_taken,\n",
    "        \"accuracy\": dict(accuracy),\n",
    "        \"num_qubits\": len(q_knn.quantum_circuit.qubits)\n",
    "    }\n",
    "    print(f\"Computing {len(iris_x_test)} points with {len(iris_x_train[:tp])} training points, took: {time_taken}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "with open('results/iris_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(iris_results_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf5227",
   "metadata": {},
   "source": [
    "# Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97c76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import iris dataset\n",
    "wine = datasets.load_wine()\n",
    "# np.c_ is the numpy concatenate function\n",
    "wine_df = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n",
    "                      columns= wine['feature_names'] + ['target'])\n",
    "\n",
    "wine_x= wine_df.iloc[:, :-1] # Data\n",
    "wine_y= wine_df.iloc[:, -1] # Class\n",
    "\n",
    "#Scale the data\n",
    "wine_x= pd.DataFrame(StandardScaler().fit_transform(wine_x))\n",
    "# Create PCA object.\n",
    "wine_num_components = 4\n",
    "wine_pca = PCA(n_components=wine_num_components)\n",
    "\n",
    "#Run PCA.\n",
    "wine_pComp=wine_pca.fit_transform(wine_x)\n",
    "\n",
    "wine_reduced_x = pd.DataFrame(data = wine_pComp\n",
    "             , columns = [f'PC {i}' for i in range(wine_num_components)])\n",
    "\n",
    "wine_x_train, wine_x_test, wine_y_train, wine_y_test= train_test_split(wine_reduced_x, wine_y,\n",
    "                                                   test_size= 0.14,\n",
    "                                                   shuffle= True, #shuffle the data to avoid bias\n",
    "                                                   random_state= 0)\n",
    "wine_x_train= np.asarray(wine_x_train) # Training Points\n",
    "wine_y_train= np.asarray(wine_y_train) # Labels\n",
    "\n",
    "wine_x_test= np.asarray(wine_x_test[:20])\n",
    "wine_y_test= np.asarray(wine_y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a431140",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 25.5241596698761\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 117.59724688529968\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 304.92507696151733\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1006.4113292694092\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 3642.2739400863647\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 76.04200839996338\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 98.31896662712097\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 307.98832178115845\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1016.4760529994965\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 3632.521505832672\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 26.167715072631836\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 146.49281668663025\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 306.99481773376465\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1013.4475734233856\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 3642.161744117737\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# import iris dataset\n",
    "wine = datasets.load_wine()\n",
    "# np.c_ is the numpy concatenate function\n",
    "wine_df = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n",
    "                      columns= wine['feature_names'] + ['target'])\n",
    "\n",
    "wine_x= wine_df.iloc[:, :-1] # Data\n",
    "wine_y= wine_df.iloc[:, -1] # Class\n",
    "\n",
    "#Scale the data\n",
    "wine_x= pd.DataFrame(StandardScaler().fit_transform(wine_x))\n",
    "# Create PCA object.\n",
    "wine_num_components = 4\n",
    "wine_pca = PCA(n_components=wine_num_components)\n",
    "\n",
    "#Run PCA.\n",
    "wine_pComp=wine_pca.fit_transform(wine_x)\n",
    "\n",
    "wine_reduced_x = pd.DataFrame(data = wine_pComp\n",
    "             , columns = [f'PC {i}' for i in range(wine_num_components)])\n",
    "\n",
    "wine_x_train, wine_x_test, wine_y_train, wine_y_test= train_test_split(wine_reduced_x, wine_y,\n",
    "                                                   test_size= 0.14,\n",
    "                                                   shuffle= True, #shuffle the data to avoid bias\n",
    "                                                   random_state= 0)\n",
    "wine_x_train= np.asarray(wine_x_train) # Training Points\n",
    "wine_y_train= np.asarray(wine_y_train) # Labels\n",
    "\n",
    "wine_x_test= np.asarray(wine_x_test[:20])\n",
    "wine_y_test= np.asarray(wine_y_test[:20])\n",
    "\n",
    "wine_results_dict = {}\n",
    "training_points = [8, 16, 32, 64, 128]\n",
    "for tp in training_points:\n",
    "    print(f\"Computing {len(wine_x_test)} points with {len(wine_x_train[:tp])} training points, started.\")\n",
    "    pred_y = defaultdict(lambda: [])\n",
    "    q_knn_results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, test_point in enumerate(wine_x_test):\n",
    "        q_knn = QuantumKNN(wine_x_train[:tp], wine_y_train[:tp], test_point)\n",
    "        q_knn.measure()\n",
    "        q_knn_results.append(q_knn)\n",
    "        for nn in range(1, tp+1):\n",
    "            if nn <= tp:\n",
    "                pred_y[str(nn)].append(q_knn.predict(nn))\n",
    "\n",
    "    #q_knn.quantum_circuit.draw('mpl', filename=f\"results/wine_quantum_circuit_{tp}_training_points.png\")\n",
    "    accuracy = defaultdict(lambda: 0)\n",
    "    for pkey in pred_y:\n",
    "        accuracy[pkey] = accuracy_score(wine_y_test, pred_y[pkey])\n",
    "    time_taken = time.time() - start_time\n",
    "    wine_results_dict[str(tp)] = {\n",
    "        \"pred_y\": dict(pred_y),\n",
    "        \"time_taken\": time_taken,\n",
    "        \"accuracy\": dict(accuracy),\n",
    "        \"num_qubits\": len(q_knn.quantum_circuit.qubits)\n",
    "    }\n",
    "    print(f\"Computing {len(wine_x_test)} points with {len(wine_x_train[:tp])} training points, took: {time_taken}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "with open('results/wine_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(wine_results_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c20dcb",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7517f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import iris dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "# np.c_ is the numpy concatenate function\n",
    "breast_cancer_df = pd.DataFrame(data= np.c_[breast_cancer['data'], breast_cancer['target']],\n",
    "                      columns= np.append(breast_cancer['feature_names'], ['target']))\n",
    "\n",
    "breast_cancer_x= breast_cancer_df.iloc[:, :-1] # Data\n",
    "breast_cancer_y= breast_cancer_df.iloc[:, -1] # Class\n",
    "\n",
    "#Scale the data\n",
    "breast_cancer_x= pd.DataFrame(StandardScaler().fit_transform(breast_cancer_x))\n",
    "# Create PCA object.\n",
    "breast_cancer_num_components = 4\n",
    "breast_cancer_pca = PCA(n_components=breast_cancer_num_components)\n",
    "\n",
    "#Run PCA.\n",
    "pComp=breast_cancer_pca.fit_transform(breast_cancer_x)\n",
    "\n",
    "breast_cancer_reduced_x = pd.DataFrame(data = pComp\n",
    "             , columns = [f'PC {i}' for i in range(breast_cancer_num_components)])\n",
    "\n",
    "breast_cancer_x_train, breast_cancer_x_test, breast_cancer_y_train, breast_cancer_y_test= train_test_split(\n",
    "    breast_cancer_reduced_x, breast_cancer_y,\n",
    "    test_size= 0.14,\n",
    "    shuffle= True, #shuffle the data to avoid bias\n",
    "    random_state= 0)\n",
    "\n",
    "breast_cancer_x_train= np.asarray(breast_cancer_x_train) # Training Points\n",
    "breast_cancer_y_train= np.asarray(breast_cancer_y_train) # Labels\n",
    "\n",
    "breast_cancer_x_test= np.asarray(breast_cancer_x_test[:20])\n",
    "breast_cancer_y_test= np.asarray(breast_cancer_y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a484e379",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 24.70741033554077\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 148.76504731178284\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 326.3528046607971\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1049.5465931892395\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 3641.968740940094\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 24.401777744293213\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 82.87090468406677\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 375.7618374824524\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1047.184784412384\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 3622.328320503235\n",
      "-------------\n",
      "Computing 20 points with 8 training points, started.\n",
      "Computing 20 points with 8 training points, took: 24.371420860290527\n",
      "-------------\n",
      "Computing 20 points with 16 training points, started.\n",
      "Computing 20 points with 16 training points, took: 130.71227097511292\n",
      "-------------\n",
      "Computing 20 points with 32 training points, started.\n",
      "Computing 20 points with 32 training points, took: 327.8151800632477\n",
      "-------------\n",
      "Computing 20 points with 64 training points, started.\n",
      "Computing 20 points with 64 training points, took: 1043.748200893402\n",
      "-------------\n",
      "Computing 20 points with 128 training points, started.\n",
      "Computing 20 points with 128 training points, took: 4061.9550879001617\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "    \n",
    "# import iris dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "# np.c_ is the numpy concatenate function\n",
    "breast_cancer_df = pd.DataFrame(data= np.c_[breast_cancer['data'], breast_cancer['target']],\n",
    "                      columns= np.append(breast_cancer['feature_names'], ['target']))\n",
    "\n",
    "breast_cancer_x= breast_cancer_df.iloc[:, :-1] # Data\n",
    "breast_cancer_y= breast_cancer_df.iloc[:, -1] # Class\n",
    "\n",
    "#Scale the data\n",
    "breast_cancer_x= pd.DataFrame(StandardScaler().fit_transform(breast_cancer_x))\n",
    "# Create PCA object.\n",
    "breast_cancer_num_components = 4\n",
    "breast_cancer_pca = PCA(n_components=breast_cancer_num_components)\n",
    "\n",
    "#Run PCA.\n",
    "pComp=breast_cancer_pca.fit_transform(breast_cancer_x)\n",
    "\n",
    "breast_cancer_reduced_x = pd.DataFrame(data = pComp\n",
    "             , columns = [f'PC {i}' for i in range(breast_cancer_num_components)])\n",
    "\n",
    "breast_cancer_x_train, breast_cancer_x_test, breast_cancer_y_train, breast_cancer_y_test= train_test_split(\n",
    "    breast_cancer_reduced_x, breast_cancer_y,\n",
    "    test_size= 0.14,\n",
    "    shuffle= True, #shuffle the data to avoid bias\n",
    "    random_state= 0)\n",
    "\n",
    "breast_cancer_x_train= np.asarray(breast_cancer_x_train) # Training Points\n",
    "breast_cancer_y_train= np.asarray(breast_cancer_y_train) # Labels\n",
    "\n",
    "breast_cancer_x_test= np.asarray(breast_cancer_x_test[:20])\n",
    "breast_cancer_y_test= np.asarray(breast_cancer_y_test[:20])\n",
    "\n",
    "breast_cancer_results_dict = {}\n",
    "training_points = [8, 16, 32, 64, 128]\n",
    "for tp in training_points:\n",
    "    print(f\"Computing {len(breast_cancer_x_test)} points with {len(breast_cancer_x_train[:tp])} training points, started.\")\n",
    "    pred_y = defaultdict(lambda: [])\n",
    "    q_knn_results = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, test_point in enumerate(breast_cancer_x_test):\n",
    "        q_knn = QuantumKNN(breast_cancer_x_train[:tp], breast_cancer_y_train[:tp], test_point)\n",
    "        q_knn.measure()\n",
    "        q_knn_results.append(q_knn)\n",
    "        for nn in range(1, tp+1):\n",
    "            if nn <= tp:\n",
    "                pred_y[str(nn)].append(q_knn.predict(nn))\n",
    "\n",
    "    #q_knn.quantum_circuit.draw('mpl', filename=f\"results/breast_cancer_quantum_circuit_{tp}_training_points.png\")\n",
    "    accuracy = defaultdict(lambda: 0)\n",
    "    for pkey in pred_y:\n",
    "        accuracy[pkey] = accuracy_score(breast_cancer_y_test, pred_y[pkey])\n",
    "    time_taken = time.time() - start_time\n",
    "    breast_cancer_results_dict[str(tp)] = {\n",
    "        \"pred_y\": dict(pred_y),\n",
    "        \"time_taken\": time_taken,\n",
    "        \"accuracy\": dict(accuracy),\n",
    "        \"num_qubits\": len(q_knn.quantum_circuit.qubits)\n",
    "    }\n",
    "    print(f\"Computing {len(breast_cancer_x_test)} points with {len(breast_cancer_x_train[:tp])} training points, took: {time_taken}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "with open('results/breast_cancer_results.pkl', 'wb') as fp:\n",
    "    pickle.dump(breast_cancer_results_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
